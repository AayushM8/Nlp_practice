{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5799ddf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "255c7b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "paragraph= \"\"\"Learning rate decay is a technique for training modern neural networks. It starts training the network with a large learning rate and then slowly reducing/decaying it until local minima is obtained. It is empirically observed to help both optimization and generalization.\n",
    "\n",
    ">When we have a constant learning rate, the steps taken by our algorithm while iterating towards minima are so noisy that after certain iterations it seems wandering around the minima and do not actually converges.\n",
    ">\n",
    ">When the learning rate is large initially we still have relatively fast learning but as tending towards minima learning rate gets smaller and smaller, end up oscillating in a tighter region around minima rather than wandering far away from it.\n",
    ">\n",
    ">Learing rate decay is implemented as follows:\"\"\"\n",
    "#tokenization\n",
    "sentences=nltk.sent_tokenize(paragraph)\n",
    "#print(sentences[1])\n",
    "\n",
    "words=nltk.word_tokenize(paragraph)\n",
    "#print(words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b005e1ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9658a259",
   "metadata": {},
   "source": [
    "#### Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "51e55d00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['learn rate decay techniqu train modern neural network .', 'it start train network larg learn rate slowli reducing/decay local minima obtain .', 'it empir observ help optim gener .', '> when constant learn rate , step taken algorithm iter toward minima noisi certain iter seem wander around minima actual converg .', '> > when learn rate larg initi still rel fast learn tend toward minima learn rate get smaller smaller , end oscil tighter region around minima rather wander far away .', '> > lear rate decay implement follow :']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "stemmer=PorterStemmer()\n",
    "#this creates an object \n",
    "\n",
    "for i in range(len(sentences)):\n",
    "    words= nltk.word_tokenize(sentences[i])\n",
    "    words= [stemmer.stem(word) for word in words if word not in set(stopwords.words('english'))]\n",
    "    #.stem is a function call from the stemmer object created which has all the methods of the porterstemmer \n",
    "    \n",
    "    sentences[i]=' '.join(words)\n",
    "print(sentences)\n",
    "#Disadvantage: many converted words don't have any actual meaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52d42f8e",
   "metadata": {},
   "source": [
    "#### Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0fe45aa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Learning rate decay technique training modern neural network .', 'It start training network large learning rate slowly reducing/decaying local minimum obtained .', 'It empirically observed help optimization generalization .', '> When constant learning rate , step taken algorithm iterating towards minimum noisy certain iteration seems wandering around minimum actually converges .', '> > When learning rate large initially still relatively fast learning tending towards minimum learning rate get smaller smaller , end oscillating tighter region around minimum rather wandering far away .', '> > Learing rate decay implemented follows :']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "lemmatizer= WordNetLemmatizer()\n",
    "#object created\n",
    "\n",
    "sentences=nltk.sent_tokenize(paragraph)\n",
    "for i in range(len(sentences)):\n",
    "    words= nltk.word_tokenize(sentences[i])\n",
    "    words= [lemmatizer.lemmatize(word) for word in words if word not in set(stopwords.words(    \n",
    "print(sentences)\n",
    "#Disadvantage: Much slower than stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f1c813",
   "metadata": {},
   "source": [
    "#### Bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d127ab12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['learning rate decay technique training modern neural network', 'start training network large learning rate slowly reducing decaying local minimum obtained', 'empirically observed help optimization generalization', 'constant learning rate step taken algorithm iterating towards minimum noisy certain iteration seems wandering around minimum actually converges', 'learning rate large initially still relatively fast learning tending towards minimum learning rate get smaller smaller end oscillating tighter region around minimum rather wandering far away', 'learing rate decay implemented follows']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "ps = PorterStemmer()\n",
    "wordnet= WordNetLemmatizer()\n",
    "#objects created\n",
    "\n",
    "corpus=[]\n",
    "\n",
    "sentences=nltk.sent_tokenize(paragraph)\n",
    "\n",
    "for i in range(len(sentences)):\n",
    "    review= re.sub('[^a-zA-Z]',' ',sentences[i])\n",
    "    #The above line removes all the characters except a-z and A-Z and replaces the with a space\n",
    "    \n",
    "    review=review.lower()\n",
    "    #the above line lowers all the characters\n",
    "    \n",
    "    review=review.split()\n",
    "    #this gives us a list of words\n",
    "    \n",
    "    review=[wordnet.lemmatize(word) for word in review if not word in set(stopwords.words('english'))]\n",
    "    # this removes all the stopwords from the given dataset\n",
    "    \n",
    "    review= ' '.join(review)\n",
    "    corpus.append(review)\n",
    "    \n",
    "print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e43c0079",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 1 1 0 0 0 0 0 1 0\n",
      "  0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0]\n",
      " [0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 1 1 0 1 0 0 0 1 0 0 1 0\n",
      "  1 0 0 0 1 0 1 0 0 0 0 0 0 0 1 0]\n",
      " [0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [1 1 1 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 1 0 2 0 0 0 1 0 0 0 0 1 0\n",
      "  0 0 0 1 0 0 0 1 0 1 0 0 0 1 0 1]\n",
      " [0 0 1 1 0 0 0 0 0 0 1 1 1 0 0 1 0 0 1 0 0 1 0 3 0 2 0 0 0 0 0 0 0 1 2 1\n",
      "  0 1 1 0 0 2 0 0 1 0 0 1 1 1 0 1]\n",
      " [0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "cv=CountVectorizer()\n",
    "x=cv.fit_transform(corpus).toarray()\n",
    "#transforming the words in corpus to vector for the machine to identify and later converting it to array\n",
    "\n",
    "print(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bc07347c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.         0.         0.         0.         0.\n",
      "  0.         0.34554249 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.24999106\n",
      "  0.         0.         0.42138593 0.34554249 0.42138593 0.\n",
      "  0.         0.         0.         0.         0.21588728 0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.42138593 0.\n",
      "  0.         0.         0.34554249 0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.33129656 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.27166792 0.         0.19654472\n",
      "  0.33129656 0.22936073 0.         0.27166792 0.         0.\n",
      "  0.         0.33129656 0.         0.         0.16973208 0.\n",
      "  0.33129656 0.         0.         0.         0.33129656 0.\n",
      "  0.33129656 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.27166792 0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.4472136  0.         0.\n",
      "  0.         0.         0.4472136  0.         0.4472136  0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.4472136  0.         0.4472136  0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.2536006  0.2536006  0.20795612 0.         0.2536006  0.2536006\n",
      "  0.2536006  0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.2536006  0.2536006  0.         0.         0.15045088\n",
      "  0.         0.3511417  0.         0.         0.         0.2536006\n",
      "  0.         0.         0.         0.         0.12992637 0.\n",
      "  0.         0.         0.         0.2536006  0.         0.\n",
      "  0.         0.2536006  0.         0.2536006  0.         0.\n",
      "  0.         0.20795612 0.         0.20795612]\n",
      " [0.         0.         0.16136387 0.19678177 0.         0.\n",
      "  0.         0.         0.         0.         0.19678177 0.19678177\n",
      "  0.19678177 0.         0.         0.19678177 0.         0.\n",
      "  0.19678177 0.         0.         0.16136387 0.         0.35022775\n",
      "  0.         0.27246893 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.19678177 0.20163312 0.19678177\n",
      "  0.         0.19678177 0.19678177 0.         0.         0.39356354\n",
      "  0.         0.         0.19678177 0.         0.         0.19678177\n",
      "  0.19678177 0.16136387 0.         0.16136387]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.41338476 0.         0.         0.         0.\n",
      "  0.         0.50411896 0.         0.         0.         0.50411896\n",
      "  0.         0.         0.         0.         0.50411896 0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.25827362 0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "cv= TfidfVectorizer()\n",
    "#created an object which will create the tf idf model which gives us the value of how common or useful the wrod is used\n",
    "\n",
    "X=cv.fit_transform(corpus).toarray()\n",
    "\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf41807",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
